{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYOC(Bring Your Own Container) - LightGBM\n",
    "----\n",
    "\n",
    "이 노트북은 SageMaker에서 지원하지 않는 LightGBM 알고리즘을 Docker 이미지로 빌드 후, Amazon ECR로 푸시하여 iris dataset에서\n",
    "간단한 학습과 추론른 수행하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (1.14.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (0.20.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage\n",
    "import lightgbm as lgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import json\n",
    "\n",
    "prefix = 'sagemaker/byom-lightgbm/'\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n",
    "bucket_name = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "먼저, 로컬에서 도커 이미지 빌드를 `docker build` 명령으로 수행합니다.\n",
    "이미지 빌드가 완료되면, 도커의 로컬 인스턴스에게 이미지를 저장할 위치를 태깅을 통해 알려주어야 합니다. \n",
    "\n",
    "```\n",
    "$ docker build <image name>\n",
    "$ docker tag <image name> <repository name> <account number>.dkr.ecr.<region>.amazonaws.com/<image name>:<tag>\n",
    "```\n",
    "저장소 이름을 지닌 이미지에 태깅을 하지 않을 경우, 도커는 기본 설정에 맞춰서 Amazon ECR이 아니라 Docker hub에 업로드하게 됩니다. Amazon SageMaker는 현재 도커 이미지를 Amazon ECR에 올리도록 되어 있습니다. 이미지를 Doker hub가 아니라 ECR에 푸시하려면, 저장소의 호스트 이름을 가지고 태깅 작업을 해야 합니다. Amazon ECR에 업로드하는 방법은, 아래 코드 셀을 참조해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  111.6kB\r",
      "\r\n",
      "Step 1/11 : FROM ubuntu:18.04\n",
      " ---> c3c304cb4f22\n",
      "Step 2/11 : RUN apt -y update && apt install -y --no-install-recommends     wget     python3-distutils     nginx     ca-certificates     libgomp1     && apt clean\n",
      " ---> Using cache\n",
      " ---> 00b6b901760e\n",
      "Step 3/11 : RUN wget https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py &&     pip install wheel numpy scipy scikit-learn pandas lightgbm flask gevent gunicorn &&     rm -rf /root/.cache\n",
      " ---> Using cache\n",
      " ---> 08b8a7b618d8\n",
      "Step 4/11 : RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> d91c6f662327\n",
      "Step 5/11 : RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> a41526b455cc\n",
      "Step 6/11 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 63ac8e60802a\n",
      "Step 7/11 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 2fdd466d522b\n",
      "Step 8/11 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> abef54f90fd3\n",
      "Step 9/11 : COPY lightgbm /opt/program\n",
      " ---> bc1a678c65ab\n",
      "Step 10/11 : WORKDIR /opt/program\n",
      " ---> Running in eab24034f86c\n",
      "Removing intermediate container eab24034f86c\n",
      " ---> 782ed95ccc34\n",
      "Step 11/11 : ENTRYPOINT [\"/usr/bin/python\"]\n",
      " ---> Running in fb7865c05f20\n",
      "Removing intermediate container fb7865c05f20\n",
      " ---> 008697b495b0\n",
      "Successfully built 008697b495b0\n",
      "Successfully tagged sagemaker-lightgbm:latest\n",
      "The push refers to repository [143656149352.dkr.ecr.us-west-2.amazonaws.com/sagemaker-lightgbm]\n",
      "7424531b706d: Preparing\n",
      "936099ca37a9: Preparing\n",
      "2428fad2f599: Preparing\n",
      "09aff022513f: Preparing\n",
      "95655a6540c0: Preparing\n",
      "28ba7458d04b: Preparing\n",
      "838a37a24627: Preparing\n",
      "a6ebef4a95c3: Preparing\n",
      "b7f7d2967507: Preparing\n",
      "838a37a24627: Waiting\n",
      "28ba7458d04b: Waiting\n",
      "b7f7d2967507: Waiting\n",
      "a6ebef4a95c3: Waiting\n",
      "936099ca37a9: Layer already exists\n",
      "2428fad2f599: Layer already exists\n",
      "09aff022513f: Layer already exists\n",
      "95655a6540c0: Layer already exists\n",
      "a6ebef4a95c3: Layer already exists\n",
      "28ba7458d04b: Layer already exists\n",
      "838a37a24627: Layer already exists\n",
      "b7f7d2967507: Layer already exists\n",
      "7424531b706d: Pushed\n",
      "latest: digest: sha256:f75639dfd82b61bdb8771e685e39c8080293995ce0196ff0c32e07bf00d855e0 size: 2198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "algorithm_name=sagemaker-lightgbm\n",
    " \n",
    "chmod +x lightgbm/train\n",
    "chmod +x lightgbm/serve\n",
    " \n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    " \n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    " \n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    " \n",
    "aws --region ${region} ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    " \n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws --region ${region} ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    " \n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    " \n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    " \n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(iris.data, iris.target, test_size=0.2, \n",
    "                                                                stratify=iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM에 최적화된 binary 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x7f4d1b6a1208>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = lgb.Dataset(train_x, label=train_y)\n",
    "#valid = lgb.Dataset(valid_x, label=valid_y) \n",
    "\n",
    "valid = train.create_valid(valid_x, label=valid_y)\n",
    "    \n",
    "train_data_local = './data/train.bin'\n",
    "valid_data_local = './data/valid.bin'\n",
    " \n",
    "train.save_binary(train_data_local)\n",
    "valid.save_binary(valid_data_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lgb用データセットを作成する\n",
    "# train = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "# # validationデータは学習用データと関連づける\n",
    "# validation = train.create_valid(validation_x, label=validation_y)\n",
    "\n",
    "# # ローカルの保存場所\n",
    "# train_data_local = './data/train.bin'\n",
    "# val_data_local = './data/validation.bin'\n",
    "\n",
    "# # バイナリ形式で保存する\n",
    "# train.save_binary(train_data_local)\n",
    "# validation.save_binary(val_data_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lgb.Dataset(train_data_local)\n",
    "valid = lgb.Dataset(valid_data_local) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lgb.Dataset(train_x, label=train_y)\n",
    "valid = lgb.Dataset(valid_x, label=valid_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'objective':'multiclass',\n",
    "    'num_class':3,\n",
    "    'verbose': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = lgb.train(\n",
    "    params=hyperparams, \n",
    "    train_set=train,\n",
    "    valid_sets=[train, valid]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train.bin ./data/valid.bin\n"
     ]
    }
   ],
   "source": [
    "train_dir = './data/'\n",
    "valid_dir = './data/'\n",
    "train_filepath = os.path.join(train_dir, 'train.bin')\n",
    "valid_filepath = os.path.join(valid_dir, 'valid.bin')\n",
    "print(train_filepath, valid_filepath)\n",
    "dtrain = lgb.Dataset(train_filepath)\n",
    "dvalid = lgb.Dataset(valid_filepath)       \n",
    "valid_list = [dtrain, dvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'objective':'multiclass',\n",
    "    'num_class':3,\n",
    "    'verbose': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = lgb.train(\n",
    "    params=hyperparams, \n",
    "    train_set=dtrain,\n",
    "    valid_sets=valid_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train.bin ./data/valid.bin\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {'num_round': 10,\n",
    "    'objective':'multiclass',\n",
    "    'num_class':3\n",
    "}\n",
    "\n",
    "\n",
    "train_filepath = os.path.join(train_dir, 'train.bin')\n",
    "valid_filepath = os.path.join(valid_dir, 'valid.bin')\n",
    "print(train_filepath, valid_filepath)\n",
    "dtrain = lgb.Dataset(train_filepath)\n",
    "dvalid = lgb.Dataset(valid_filepath)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = lgb.train(\n",
    "    params=hyperparams, \n",
    "    train_set=dtrain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_s3 = sess.upload_data(train_data_local, key_prefix=path.join(prefix, 'input/train'), bucket=bucket_name)\n",
    "valid_data_s3 = sess.upload_data(valid_data_local, key_prefix=path.join(prefix, 'input/valid'), bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    num_round = 10,\n",
    "    objective = 'multiclass',\n",
    "    num_class = len(iris.target_names)\n",
    ")\n",
    "metric_definitions = [dict(\n",
    "    Name = 'multilogloss',\n",
    "    Regex = '.*\\\\[[0-9]+\\\\].*valid_[0-9]+\\'s\\\\smulti_logloss: (\\\\S+)'\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-143656149352/sagemaker/byom-lightgbm/output\n",
      "2020-06-13 14:55:11 Starting - Starting the training job...\n",
      "2020-06-13 14:55:13 Starting - Launching requested ML instances."
     ]
    }
   ],
   "source": [
    "modelartifact_path = \"s3://\"+path.join(bucket_name, prefix, 'output')\n",
    "print(modelartifact_path)\n",
    "model = sage.estimator.Estimator(\n",
    "    '{}.dkr.ecr.{}.amazonaws.com/sagemaker-lightgbm:latest'.format(account, region),\n",
    "    role,\n",
    "    1, # number of instances\n",
    "    'ml.c4.2xlarge', \n",
    "    output_path=modelartifact_path,\n",
    "    sagemaker_session=sess,\n",
    "    metric_definitions=metric_definitions\n",
    ")\n",
    " \n",
    "model.set_hyperparameters(**params)\n",
    " \n",
    "model.fit(dict(\n",
    "    train = train_data_s3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://practice-daekeun/sagemaker/byom-lightgbm/output\n",
      "2019-08-15 08:38:09 Starting - Starting the training job...\n",
      "2019-08-15 08:38:11 Starting - Launching requested ML instances......\n",
      "2019-08-15 08:39:16 Starting - Preparing the instances for training...\n",
      "2019-08-15 08:40:03 Downloading - Downloading input data...\n",
      "2019-08-15 08:40:38 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mStarting the training.\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\u001b[0m\n",
      "\u001b[31m[LightGBM] [Info] Total Bins 85\u001b[0m\n",
      "\u001b[31m[LightGBM] [Info] Number of data: 120, number of used features: 4\u001b[0m\n",
      "\u001b[31m[LightGBM] [Info] Start training from score -1.098612\u001b[0m\n",
      "\u001b[31m[LightGBM] [Info] Start training from score -1.098612\u001b[0m\n",
      "\u001b[31m[LightGBM] [Info] Start training from score -1.098612\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[1]#011training's multi_logloss: 0.976374#011valid_1's multi_logloss: 0.979219\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[2]#011training's multi_logloss: 0.873532#011valid_1's multi_logloss: 0.878614\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[3]#011training's multi_logloss: 0.786093#011valid_1's multi_logloss: 0.79283\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[4]#011training's multi_logloss: 0.710648#011valid_1's multi_logloss: 0.715246\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[5]#011training's multi_logloss: 0.645243#011valid_1's multi_logloss: 0.651137\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[6]#011training's multi_logloss: 0.587736#011valid_1's multi_logloss: 0.595743\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[7]#011training's multi_logloss: 0.537214#011valid_1's multi_logloss: 0.54225\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[8]#011training's multi_logloss: 0.492179#011valid_1's multi_logloss: 0.499798\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[9]#011training's multi_logloss: 0.452294#011valid_1's multi_logloss: 0.461313\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[31m[10]#011training's multi_logloss: 0.416786#011valid_1's multi_logloss: 0.427236\u001b[0m\n",
      "\u001b[31mTraining complete.\u001b[0m\n",
      "\n",
      "2019-08-15 08:40:50 Uploading - Uploading generated training model\n",
      "2019-08-15 08:40:50 Completed - Training job completed\n",
      "Billable seconds: 48\n"
     ]
    }
   ],
   "source": [
    "modelartifact_path = \"s3://\"+path.join(bucket_name, prefix, 'output')\n",
    "print(modelartifact_path)\n",
    "model = sage.estimator.Estimator(\n",
    "    '{}.dkr.ecr.{}.amazonaws.com/sagemaker-lightgbm:latest'.format(account, region),\n",
    "    role,\n",
    "    1, # number of instances\n",
    "    'ml.c4.2xlarge', \n",
    "    output_path=modelartifact_path,\n",
    "    sagemaker_session=sess,\n",
    "    metric_definitions=metric_definitions\n",
    ")\n",
    " \n",
    "model.set_hyperparameters(**params)\n",
    " \n",
    "model.fit(dict(\n",
    "    train = train_data_s3,\n",
    "    valid = valid_data_s3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = model.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [[0.7257590295228772, 0.13655327430896527, 0.13768769616815751],\n",
       "  [0.15715624612525764, 0.6611616462968629, 0.1816821075778795],\n",
       "  [0.1416324830383643, 0.14817439774286004, 0.7101931192187756],\n",
       "  [0.7257590295228772, 0.13655327430896527, 0.13768769616815751],\n",
       "  [0.15157895240496283, 0.6877240205579943, 0.16069702703704303],\n",
       "  [0.7012227664352552, 0.14751818565326474, 0.15125904791148015],\n",
       "  [0.15157895240496283, 0.6877240205579943, 0.16069702703704303],\n",
       "  [0.1418854741073229, 0.2585667619465168, 0.5995477639461603],\n",
       "  [0.15157895240496283, 0.6877240205579943, 0.16069702703704303],\n",
       "  [0.1486892054284723, 0.39355273240652433, 0.4577580621650033],\n",
       "  [0.15805570862923718, 0.17406798279313812, 0.6678763085776247],\n",
       "  [0.7012227664352552, 0.14751818565326474, 0.15125904791148015],\n",
       "  [0.7006628780463077, 0.14740040037409344, 0.15193672157959892],\n",
       "  [0.13417928359075812, 0.35296708925318, 0.5128536271560619],\n",
       "  [0.1369826720223735, 0.1489770643551188, 0.7140402636225076],\n",
       "  [0.1462150459706887, 0.7073364518830514, 0.14644850214625985],\n",
       "  [0.7006628780463077, 0.14740040037409344, 0.15193672157959892],\n",
       "  [0.7006628780463077, 0.14740040037409344, 0.15193672157959892],\n",
       "  [0.7257590295228772, 0.13655327430896527, 0.13768769616815751],\n",
       "  [0.7257590295228772, 0.13655327430896527, 0.13768769616815751],\n",
       "  [0.15805570862923718, 0.17406798279313812, 0.6678763085776247],\n",
       "  [0.15157895240496283, 0.6877240205579943, 0.16069702703704303],\n",
       "  [0.1416324830383643, 0.14817439774286004, 0.7101931192187756],\n",
       "  [0.1632854941554057, 0.17298674947840326, 0.6637277563661911],\n",
       "  [0.1462150459706887, 0.7073364518830514, 0.14644850214625985],\n",
       "  [0.1416324830383643, 0.14817439774286004, 0.7101931192187756],\n",
       "  [0.1522231181571734, 0.6872018644159797, 0.1605750174268468],\n",
       "  [0.1416324830383643, 0.14817439774286004, 0.7101931192187756],\n",
       "  [0.7012227664352552, 0.14751818565326474, 0.15125904791148015],\n",
       "  [0.15965040282139525, 0.655784093895138, 0.18456550328346666]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predictor.predict(validation_x)\n",
    "result = json.loads(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
